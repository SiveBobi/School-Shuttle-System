
# 🪞 Reflection: Challenges in Translating Requirements to Use Cases and Test Cases

Translating system and stakeholder requirements into actionable use cases and test scenarios was both insightful and challenging. The process required careful analysis, prioritization, and clear communication across technical and non-technical expectations.

---

## 🧩 Challenge 1: Interpreting Stakeholder Language

One of the biggest hurdles was **understanding vague or informal stakeholder requests**. For example, statements like “we want to track the shuttle easily” had to be broken down into specific use cases like *“View Real-Time Shuttle Location”* and *“Receive Notifications.”* This required continuous back-and-forth to interpret needs accurately and avoid assumptions.

---

## 🔄 Challenge 2: Aligning Use Cases with Functional Requirements

Maintaining **traceability between functional requirements and use cases** was critical but tricky. I had to ensure each use case mapped clearly to one or more functional requirements from Assignment 4. This helped ensure that every business goal was represented in the system model — but it also meant repeatedly cross-checking documentation for consistency.

---

## 🧠 Challenge 3: Avoiding Redundancy in Use Cases

Some functionalities were overlapping or similar, like “View Assigned Routes” and “Update Shuttle Status.” I had to carefully word each use case so that it remained **distinct yet complete**, avoiding duplication but not leaving out any important system behavior. It emphasized the importance of clarity and modular thinking in use case design.

---

## 🧪 Challenge 4: Designing Realistic Test Scenarios

Writing test cases required a shift in perspective — from designing features to **validating them under real-world conditions**. Functional test cases had to be practical, precise, and executable. The challenge was creating steps that reflected how users would interact with the system, not just how developers thought it would work.

For non-functional tests, it was even more difficult. Simulating **performance and security scenarios** required some technical assumptions (e.g., using tools like JMeter, simulating 1000 users, etc.). These tests helped me think beyond normal operations and consider scalability and reliability.

---

## 📊 Challenge 5: Balancing Simplicity and Completeness

There was a constant tension between keeping diagrams and flows **simple enough to understand** and **detailed enough to be useful**. I had to limit the number of actors and use cases shown in the diagram while still covering key scenarios.

For example, instead of breaking down all types of alerts, I grouped them under broader use cases like *“Receive Emergency Alerts.”* This helped reduce clutter but required detailed explanation elsewhere to avoid ambiguity.

---

## 🛠 Lessons Learned

- **Traceability matters**: Linking requirements → use cases → test cases helps maintain project alignment.
- **Stakeholder clarity is everything**: Misinterpreting even a small detail can lead to flawed designs.
- **Non-functional requirements are just as important**: They often reveal hidden risks in performance, security, or scalability.
- **Documentation drives quality**: Well-written flows and test cases make development and QA much easier.

---

In conclusion, this assignment deepened my understanding of how real-world systems are modeled and validated. The challenges I faced were valuable learning moments that highlighted the importance of structure, clarity, and validation in software engineering. It also reinforced how critical it is to maintain stakeholder alignment at every stage of system development.
